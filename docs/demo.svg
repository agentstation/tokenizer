<svg xmlns="http://www.w3.org/2000/svg" width="900" height="300"><rect width="900" height="300" rx="6" fill="#101216"/><g id="window-bar"><path d="M 6,0 L 894,0 Q 900,0 900,6 L 900,30 L 0,30 L 0,6 Q 0,0 6,0 Z" fill="#101216"/><circle cx="20" cy="15" r="6" fill="#ff5f58"/><circle cx="40" cy="15" r="6" fill="#ffbd2e"/><circle cx="60" cy="15" r="6" fill="#18c132"/><text x="450" y="20" text-anchor="middle" font-family="JetBrains Mono, monospace" font-size="13" fill="#cccccc">Terminal</text></g><svg x="15" y="45" width="870" height="240" viewBox="0 0 870 240"><rect width="870" height="240" fill="#101216"/><style>@keyframes slide {  0% { transform: translateX(0px); }  0.1% { transform: translateX(-870px); }  0.2% { transform: translateX(-1740px); }  0.4% { transform: translateX(-2610px); }  0.5% { transform: translateX(-3480px); }  0.7% { transform: translateX(-4350px); }  0.8% { transform: translateX(-5220px); }  1% { transform: translateX(-6090px); }  1.1% { transform: translateX(-6960px); }  1.3% { transform: translateX(-7830px); }  1.4% { transform: translateX(-8700px); }  1.6% { transform: translateX(-9570px); }  1.8% { transform: translateX(-10440px); }  1.9% { transform: translateX(-11310px); }  2.1% { transform: translateX(-12180px); }  2.2% { transform: translateX(-13050px); }  2.4% { transform: translateX(-13920px); }  2.5% { transform: translateX(-14790px); }  2.7% { transform: translateX(-15660px); }  2.8% { transform: translateX(-16530px); }  3% { transform: translateX(-17400px); }  3.1% { transform: translateX(-18270px); }  3.3% { transform: translateX(-19140px); }  3.3% { transform: translateX(-20010px); }  3.4% { transform: translateX(-20880px); }  3.6% { transform: translateX(-21750px); }  3.7% { transform: translateX(-22620px); }  3.9% { transform: translateX(-23490px); }  4% { transform: translateX(-24360px); }  4.2% { transform: translateX(-25230px); }  4.3% { transform: translateX(-26100px); }  4.5% { transform: translateX(-26970px); }  4.6% { transform: translateX(-27840px); }  4.8% { transform: translateX(-28710px); }  4.9% { transform: translateX(-29580px); }  5.1% { transform: translateX(-30450px); }  5.3% { transform: translateX(-31320px); }  5.4% { transform: translateX(-32190px); }  5.6% { transform: translateX(-33060px); }  5.7% { transform: translateX(-33930px); }  5.9% { transform: translateX(-34800px); }  6% { transform: translateX(-35670px); }  6.2% { transform: translateX(-36540px); }  6.3% { transform: translateX(-37410px); }  6.5% { transform: translateX(-38280px); }  6.6% { transform: translateX(-39150px); }  6.8% { transform: translateX(-40020px); }  6.9% { transform: translateX(-40890px); }  7.1% { transform: translateX(-41760px); }  7.2% { transform: translateX(-42630px); }  7.4% { transform: translateX(-43500px); }  7.5% { transform: translateX(-44370px); }  7.7% { transform: translateX(-45240px); }  7.8% { transform: translateX(-46110px); }  8% { transform: translateX(-46980px); }  8.1% { transform: translateX(-47850px); }  8.3% { transform: translateX(-48720px); }  8.4% { transform: translateX(-49590px); }  8.6% { transform: translateX(-50460px); }  8.7% { transform: translateX(-51330px); }  8.9% { transform: translateX(-52200px); }  9% { transform: translateX(-53070px); }  9.2% { transform: translateX(-53940px); }  9.3% { transform: translateX(-54810px); }  9.5% { transform: translateX(-55680px); }  9.6% { transform: translateX(-56550px); }  9.8% { transform: translateX(-57420px); }  9.9% { transform: translateX(-58290px); }  10.1% { transform: translateX(-59160px); }  10.2% { transform: translateX(-60030px); }  10.4% { transform: translateX(-60900px); }  10.5% { transform: translateX(-61770px); }  10.6% { transform: translateX(-62640px); }  10.7% { transform: translateX(-63510px); }  10.8% { transform: translateX(-64380px); }  11% { transform: translateX(-65250px); }  11.1% { transform: translateX(-66120px); }  11.3% { transform: translateX(-66990px); }  11.5% { transform: translateX(-67860px); }  11.6% { transform: translateX(-68730px); }  13.1% { transform: translateX(-69600px); }  17.4% { transform: translateX(-70470px); }  17.6% { transform: translateX(-71340px); }  17.7% { transform: translateX(-72210px); }  17.8% { transform: translateX(-73080px); }  17.9% { transform: translateX(-73950px); }  18% { transform: translateX(-74820px); }  18.1% { transform: translateX(-75690px); }  18.2% { transform: translateX(-76560px); }  18.3% { transform: translateX(-77430px); }  18.5% { transform: translateX(-78300px); }  18.6% { transform: translateX(-79170px); }  18.8% { transform: translateX(-80040px); }  18.9% { transform: translateX(-80910px); }  19.1% { transform: translateX(-81780px); }  19.2% { transform: translateX(-82650px); }  19.4% { transform: translateX(-83520px); }  19.5% { transform: translateX(-84390px); }  19.7% { transform: translateX(-85260px); }  19.8% { transform: translateX(-86130px); }  20% { transform: translateX(-87000px); }  20.1% { transform: translateX(-87870px); }  20.3% { transform: translateX(-88740px); }  20.4% { transform: translateX(-89610px); }  20.6% { transform: translateX(-90480px); }  20.7% { transform: translateX(-91350px); }  20.9% { transform: translateX(-92220px); }  21% { transform: translateX(-93090px); }  21.2% { transform: translateX(-93960px); }  21.3% { transform: translateX(-94830px); }  21.5% { transform: translateX(-95700px); }  21.6% { transform: translateX(-96570px); }  21.8% { transform: translateX(-97440px); }  21.9% { transform: translateX(-98310px); }  22.1% { transform: translateX(-99180px); }  22.2% { transform: translateX(-100050px); }  22.4% { transform: translateX(-100920px); }  22.5% { transform: translateX(-101790px); }  22.7% { transform: translateX(-102660px); }  22.8% { transform: translateX(-103530px); }  23% { transform: translateX(-104400px); }  23.1% { transform: translateX(-105270px); }  23.3% { transform: translateX(-106140px); }  23.4% { transform: translateX(-107010px); }  23.6% { transform: translateX(-107880px); }  23.7% { transform: translateX(-108750px); }  23.9% { transform: translateX(-109620px); }  24% { transform: translateX(-110490px); }  24.2% { transform: translateX(-111360px); }  24.3% { transform: translateX(-112230px); }  24.5% { transform: translateX(-113100px); }  24.6% { transform: translateX(-113970px); }  24.8% { transform: translateX(-114840px); }  24.9% { transform: translateX(-115710px); }  25.1% { transform: translateX(-116580px); }  25.2% { transform: translateX(-117450px); }  25.4% { transform: translateX(-118320px); }  25.5% { transform: translateX(-119190px); }  25.7% { transform: translateX(-120060px); }  25.8% { transform: translateX(-120930px); }  26% { transform: translateX(-121800px); }  26.1% { transform: translateX(-122670px); }  26.3% { transform: translateX(-123540px); }  26.4% { transform: translateX(-124410px); }  26.6% { transform: translateX(-125280px); }  26.7% { transform: translateX(-126150px); }  26.9% { transform: translateX(-127020px); }  26.9% { transform: translateX(-127890px); }  27% { transform: translateX(-128760px); }  27.2% { transform: translateX(-129630px); }  27.3% { transform: translateX(-130500px); }  27.5% { transform: translateX(-131370px); }  27.6% { transform: translateX(-132240px); }  27.8% { transform: translateX(-133110px); }  27.9% { transform: translateX(-133980px); }  28.1% { transform: translateX(-134850px); }  28.2% { transform: translateX(-135720px); }  28.4% { transform: translateX(-136590px); }  28.5% { transform: translateX(-137460px); }  28.7% { transform: translateX(-138330px); }  28.8% { transform: translateX(-139200px); }  29% { transform: translateX(-140070px); }  29.1% { transform: translateX(-140940px); }  29.3% { transform: translateX(-141810px); }  29.5% { transform: translateX(-142680px); }  29.6% { transform: translateX(-143550px); }  29.8% { transform: translateX(-144420px); }  29.9% { transform: translateX(-145290px); }  30.1% { transform: translateX(-146160px); }  30.2% { transform: translateX(-147030px); }  30.4% { transform: translateX(-147900px); }  30.5% { transform: translateX(-148770px); }  30.7% { transform: translateX(-149640px); }  30.8% { transform: translateX(-150510px); }  31% { transform: translateX(-151380px); }  31.1% { transform: translateX(-152250px); }  31.3% { transform: translateX(-153120px); }  31.4% { transform: translateX(-153990px); }  31.6% { transform: translateX(-154860px); }  33.1% { transform: translateX(-155730px); }  37.4% { transform: translateX(-156600px); }  37.4% { transform: translateX(-157470px); }  37.5% { transform: translateX(-158340px); }  37.7% { transform: translateX(-159210px); }  37.7% { transform: translateX(-160080px); }  37.8% { transform: translateX(-160950px); }  38% { transform: translateX(-161820px); }  38% { transform: translateX(-162690px); }  38.1% { transform: translateX(-163560px); }  38.3% { transform: translateX(-164430px); }  38.3% { transform: translateX(-165300px); }  38.4% { transform: translateX(-166170px); }  38.6% { transform: translateX(-167040px); }  38.6% { transform: translateX(-167910px); }  38.7% { transform: translateX(-168780px); }  38.9% { transform: translateX(-169650px); }  39% { transform: translateX(-170520px); }  39.2% { transform: translateX(-171390px); }  39.3% { transform: translateX(-172260px); }  39.5% { transform: translateX(-173130px); }  39.6% { transform: translateX(-174000px); }  39.8% { transform: translateX(-174870px); }  40% { transform: translateX(-175740px); }  40.1% { transform: translateX(-176610px); }  40.3% { transform: translateX(-177480px); }  40.4% { transform: translateX(-178350px); }  40.6% { transform: translateX(-179220px); }  40.7% { transform: translateX(-180090px); }  40.9% { transform: translateX(-180960px); }  41% { transform: translateX(-181830px); }  41.2% { transform: translateX(-182700px); }  41.3% { transform: translateX(-183570px); }  41.5% { transform: translateX(-184440px); }  41.6% { transform: translateX(-185310px); }  41.8% { transform: translateX(-186180px); }  41.9% { transform: translateX(-187050px); }  42.1% { transform: translateX(-187920px); }  42.2% { transform: translateX(-188790px); }  42.4% { transform: translateX(-189660px); }  42.5% { transform: translateX(-190530px); }  42.7% { transform: translateX(-191400px); }  42.8% { transform: translateX(-192270px); }  43% { transform: translateX(-193140px); }  43% { transform: translateX(-194010px); }  43.2% { transform: translateX(-194880px); }  43.3% { transform: translateX(-195750px); }  43.5% { transform: translateX(-196620px); }  43.6% { transform: translateX(-197490px); }  43.8% { transform: translateX(-198360px); }  43.9% { transform: translateX(-199230px); }  44.1% { transform: translateX(-200100px); }  44.2% { transform: translateX(-200970px); }  44.4% { transform: translateX(-201840px); }  44.5% { transform: translateX(-202710px); }  44.7% { transform: translateX(-203580px); }  44.8% { transform: translateX(-204450px); }  45% { transform: translateX(-205320px); }  45.1% { transform: translateX(-206190px); }  45.3% { transform: translateX(-207060px); }  45.4% { transform: translateX(-207930px); }  45.6% { transform: translateX(-208800px); }  45.7% { transform: translateX(-209670px); }  45.9% { transform: translateX(-210540px); }  46% { transform: translateX(-211410px); }  46.2% { transform: translateX(-212280px); }  46.3% { transform: translateX(-213150px); }  46.5% { transform: translateX(-214020px); }  46.6% { transform: translateX(-214890px); }  46.8% { transform: translateX(-215760px); }  46.9% { transform: translateX(-216630px); }  47.1% { transform: translateX(-217500px); }  47.2% { transform: translateX(-218370px); }  47.4% { transform: translateX(-219240px); }  47.5% { transform: translateX(-220110px); }  47.7% { transform: translateX(-220980px); }  47.8% { transform: translateX(-221850px); }  47.9% { transform: translateX(-222720px); }  48% { transform: translateX(-223590px); }  48.1% { transform: translateX(-224460px); }  48.3% { transform: translateX(-225330px); }  48.4% { transform: translateX(-226200px); }  48.6% { transform: translateX(-227070px); }  48.7% { transform: translateX(-227940px); }  48.9% { transform: translateX(-228810px); }  49% { transform: translateX(-229680px); }  49.2% { transform: translateX(-230550px); }  49.3% { transform: translateX(-231420px); }  49.5% { transform: translateX(-232290px); }  49.6% { transform: translateX(-233160px); }  49.8% { transform: translateX(-234030px); }  49.9% { transform: translateX(-234900px); }  50.1% { transform: translateX(-235770px); }  50.2% { transform: translateX(-236640px); }  50.4% { transform: translateX(-237510px); }  50.5% { transform: translateX(-238380px); }  50.7% { transform: translateX(-239250px); }  50.8% { transform: translateX(-240120px); }  51% { transform: translateX(-240990px); }  51.1% { transform: translateX(-241860px); }  51.3% { transform: translateX(-242730px); }  51.4% { transform: translateX(-243600px); }  51.6% { transform: translateX(-244470px); }  51.7% { transform: translateX(-245340px); }  51.9% { transform: translateX(-246210px); }  52% { transform: translateX(-247080px); }  52.2% { transform: translateX(-247950px); }  52.4% { transform: translateX(-248820px); }  52.4% { transform: translateX(-249690px); }  52.5% { transform: translateX(-250560px); }  54% { transform: translateX(-251430px); }  58.3% { transform: translateX(-252300px); }  58.4% { transform: translateX(0px); }  58.5% { transform: translateX(-870px); }  58.6% { transform: translateX(-253170px); }  58.7% { transform: translateX(-1740px); }  58.8% { transform: translateX(-254040px); }  58.9% { transform: translateX(-254910px); }  59% { transform: translateX(-255780px); }  59.1% { transform: translateX(-256650px); }  59.3% { transform: translateX(-257520px); }  59.4% { transform: translateX(-258390px); }  59.5% { transform: translateX(-259260px); }  59.6% { transform: translateX(-260130px); }  59.7% { transform: translateX(-261000px); }  59.8% { transform: translateX(-261870px); }  59.9% { transform: translateX(-262740px); }  60% { transform: translateX(-263610px); }  60.2% { transform: translateX(-264480px); }  60.3% { transform: translateX(-265350px); }  60.4% { transform: translateX(-266220px); }  60.5% { transform: translateX(-267090px); }  60.6% { transform: translateX(-267960px); }  60.7% { transform: translateX(-268830px); }  60.8% { transform: translateX(-269700px); }  60.9% { transform: translateX(-270570px); }  61.1% { transform: translateX(-271440px); }  61.2% { transform: translateX(-272310px); }  61.4% { transform: translateX(-273180px); }  61.5% { transform: translateX(-274050px); }  61.7% { transform: translateX(-274920px); }  61.8% { transform: translateX(-275790px); }  62% { transform: translateX(-276660px); }  62.1% { transform: translateX(-277530px); }  62.3% { transform: translateX(-278400px); }  62.4% { transform: translateX(-279270px); }  62.6% { transform: translateX(-280140px); }  62.7% { transform: translateX(-281010px); }  62.9% { transform: translateX(-281880px); }  63% { transform: translateX(-282750px); }  63.2% { transform: translateX(-283620px); }  63.3% { transform: translateX(-284490px); }  63.5% { transform: translateX(-285360px); }  63.6% { transform: translateX(-286230px); }  63.8% { transform: translateX(-287100px); }  63.9% { transform: translateX(-287970px); }  64.1% { transform: translateX(-288840px); }  64.2% { transform: translateX(-289710px); }  64.4% { transform: translateX(-290580px); }  64.5% { transform: translateX(-291450px); }  64.7% { transform: translateX(-292320px); }  64.8% { transform: translateX(-293190px); }  65% { transform: translateX(-294060px); }  65.1% { transform: translateX(-294930px); }  65.3% { transform: translateX(-295800px); }  65.4% { transform: translateX(-296670px); }  65.6% { transform: translateX(-297540px); }  65.7% { transform: translateX(-298410px); }  65.9% { transform: translateX(-299280px); }  66% { transform: translateX(-300150px); }  66.2% { transform: translateX(-301020px); }  66.3% { transform: translateX(-301890px); }  66.4% { transform: translateX(-302760px); }  66.5% { transform: translateX(-303630px); }  66.6% { transform: translateX(-304500px); }  66.8% { transform: translateX(-305370px); }  66.9% { transform: translateX(-306240px); }  67.1% { transform: translateX(-307110px); }  67.2% { transform: translateX(-307980px); }  67.4% { transform: translateX(-308850px); }  67.5% { transform: translateX(-309720px); }  67.7% { transform: translateX(-310590px); }  67.8% { transform: translateX(-311460px); }  68% { transform: translateX(-312330px); }  68.1% { transform: translateX(-313200px); }  68.3% { transform: translateX(-314070px); }  68.3% { transform: translateX(-314940px); }  68.4% { transform: translateX(-315810px); }  68.6% { transform: translateX(-316680px); }  68.6% { transform: translateX(-317550px); }  68.7% { transform: translateX(-318420px); }  68.9% { transform: translateX(-319290px); }  68.9% { transform: translateX(-320160px); }  69% { transform: translateX(-321030px); }  69.2% { transform: translateX(-321900px); }  69.3% { transform: translateX(-322770px); }  69.5% { transform: translateX(-323640px); }  69.5% { transform: translateX(-324510px); }  69.6% { transform: translateX(-325380px); }  69.8% { transform: translateX(-326250px); }  69.8% { transform: translateX(-327120px); }  69.9% { transform: translateX(-327990px); }  70.1% { transform: translateX(-328860px); }  70.2% { transform: translateX(-329730px); }  70.4% { transform: translateX(-330600px); }  71.9% { transform: translateX(-331470px); }  76.2% { transform: translateX(-332340px); }  76.4% { transform: translateX(-333210px); }  76.5% { transform: translateX(-334080px); }  76.6% { transform: translateX(-334950px); }  76.7% { transform: translateX(-335820px); }  76.8% { transform: translateX(-336690px); }  76.9% { transform: translateX(-337560px); }  77% { transform: translateX(-338430px); }  77.1% { transform: translateX(-339300px); }  77.2% { transform: translateX(-340170px); }  77.3% { transform: translateX(-341040px); }  77.4% { transform: translateX(-341910px); }  77.5% { transform: translateX(-342780px); }  77.6% { transform: translateX(-343650px); }  77.7% { transform: translateX(-344520px); }  77.9% { transform: translateX(-345390px); }  78% { transform: translateX(-346260px); }  78.2% { transform: translateX(-347130px); }  78.2% { transform: translateX(-348000px); }  78.3% { transform: translateX(-348870px); }  78.5% { transform: translateX(-349740px); }  78.5% { transform: translateX(-350610px); }  78.6% { transform: translateX(-351480px); }  78.7% { transform: translateX(-352350px); }  78.8% { transform: translateX(-353220px); }  78.8% { transform: translateX(-354090px); }  78.9% { transform: translateX(-354960px); }  79.1% { transform: translateX(-355830px); }  79.2% { transform: translateX(-356700px); }  79.4% { transform: translateX(-357570px); }  79.5% { transform: translateX(-358440px); }  79.7% { transform: translateX(-359310px); }  79.8% { transform: translateX(-360180px); }  80% { transform: translateX(-361050px); }  80.1% { transform: translateX(-361920px); }  80.3% { transform: translateX(-362790px); }  80.4% { transform: translateX(-363660px); }  80.6% { transform: translateX(-364530px); }  80.7% { transform: translateX(-365400px); }  80.9% { transform: translateX(-366270px); }  81.1% { transform: translateX(-367140px); }  81.2% { transform: translateX(-368010px); }  81.4% { transform: translateX(-368880px); }  81.5% { transform: translateX(-369750px); }  81.7% { transform: translateX(-370620px); }  81.8% { transform: translateX(-371490px); }  82% { transform: translateX(-372360px); }  82.1% { transform: translateX(-373230px); }  82.3% { transform: translateX(-374100px); }  82.4% { transform: translateX(-374970px); }  82.6% { transform: translateX(-375840px); }  82.7% { transform: translateX(-376710px); }  82.8% { transform: translateX(-377580px); }  82.9% { transform: translateX(-378450px); }  83% { transform: translateX(-379320px); }  83.2% { transform: translateX(-380190px); }  83.3% { transform: translateX(-381060px); }  83.5% { transform: translateX(-381930px); }  83.6% { transform: translateX(-382800px); }  85.1% { transform: translateX(-383670px); }  89.4% { transform: translateX(-384540px); }  89.6% { transform: translateX(-385410px); }  89.7% { transform: translateX(-386280px); }  89.9% { transform: translateX(-387150px); }  90% { transform: translateX(-388020px); }  90.2% { transform: translateX(-388890px); }  90.3% { transform: translateX(-389760px); }  90.5% { transform: translateX(-390630px); }  90.6% { transform: translateX(-391500px); }  90.8% { transform: translateX(-392370px); }  90.9% { transform: translateX(-393240px); }  91.1% { transform: translateX(-394110px); }  91.2% { transform: translateX(-394980px); }  91.4% { transform: translateX(-395850px); }  91.6% { transform: translateX(-396720px); }  91.7% { transform: translateX(-397590px); }  91.9% { transform: translateX(-398460px); }  92% { transform: translateX(-399330px); }  92.2% { transform: translateX(-400200px); }  92.3% { transform: translateX(-401070px); }  92.5% { transform: translateX(-401940px); }  92.6% { transform: translateX(-402810px); }  92.8% { transform: translateX(-403680px); }  92.9% { transform: translateX(-404550px); }  93.1% { transform: translateX(-405420px); }  93.2% { transform: translateX(-406290px); }  93.4% { transform: translateX(-407160px); }  93.5% { transform: translateX(-408030px); }  93.7% { transform: translateX(-408900px); }  93.8% { transform: translateX(-409770px); }  94% { transform: translateX(-410640px); }  94.1% { transform: translateX(-411510px); }  94.3% { transform: translateX(-412380px); }  94.4% { transform: translateX(-413250px); }  94.6% { transform: translateX(-414120px); }  94.7% { transform: translateX(-414990px); }  94.9% { transform: translateX(-415860px); }  95% { transform: translateX(-416730px); }  95.2% { transform: translateX(-417600px); }  95.3% { transform: translateX(-418470px); }  95.5% { transform: translateX(-419340px); }  95.6% { transform: translateX(-420210px); }  95.8% { transform: translateX(-421080px); }  95.9% { transform: translateX(-421950px); }  96.1% { transform: translateX(-422820px); }  96.2% { transform: translateX(-423690px); }  96.4% { transform: translateX(-424560px); }  96.5% { transform: translateX(-425430px); }  96.7% { transform: translateX(-426300px); }  96.8% { transform: translateX(-427170px); }  97% { transform: translateX(-428040px); }  98.5% { transform: translateX(-428910px); }  100% { transform: translateX(-428910px); }}.animation-container {  animation: slide 33.16s step-end 0s infinite;}.t { fill: #8b949e; font-family: JetBrains Mono, monospace; font-size: 13px; }.c { fill: #2b7489; }.w { fill: #ffffff; }.k { fill: #000000; }.r { fill: #f78166; }.g { fill: #56d364; }.y { fill: #e3b341; }.b { fill: #6ca4f8; }.m { fill: #db61a2; }.p { fill: #6ca4f8; }.ca { }@keyframes blink { 0%, 49% { opacity: 1; } 50%, 100% { opacity: 0; } }.ci { animation: blink 1s infinite; }</style><defs></defs><g class="animation-container"><g transform="translate(0,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">#</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(870,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(1740,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># T</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(2610,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># To</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(3480,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(4350,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(5220,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(6090,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(6960,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(7830,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(8700,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(9570,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize P</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(10440,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Py</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(11310,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Pyt</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(12180,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Pyth</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(13050,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Pytho</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(13920,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(14790,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(15660,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python c</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(16530,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python co</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(17400,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python cod</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(18270,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(19140,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text><text y="30.6" xml:space="preserve"><tspan x="0" class="t">&gt;</tspan></text></g><g transform="translate(20010,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(20880,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(21750,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">ec</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(22620,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">ech</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(23490,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(24360,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(25230,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(26100,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;d</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(26970,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;de</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(27840,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(28710,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(29580,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def a</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(30450,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def ad</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(31320,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(32190,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(33060,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(33930,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a,</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(34800,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(35670,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(36540,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b)</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(37410,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b):</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(38280,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(39150,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): r</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(40020,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): re</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(40890,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): ret</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(41760,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): retu</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(42630,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): retur</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(43500,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(44370,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(45240,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(46110,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(46980,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a +</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(47850,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(48720,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(49590,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(50460,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(51330,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; |</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(52200,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(53070,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(53940,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(54810,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(55680,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(56550,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(57420,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(58290,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(59160,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(60030,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(60900,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(61770,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(62640,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(63510,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(64380,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer lla</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(65250,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(66120,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(66990,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(67860,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan x="0"></tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(68730,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(69600,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ci" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(70470,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">#</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(71340,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(72210,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># C</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(73080,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># C</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(73950,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Co</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(74820,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Cou</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(75690,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Cou</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(76560,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Coun</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(77430,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(78300,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(79170,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(80040,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(80910,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(81780,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(82650,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(83520,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(84390,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(85260,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens i</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(86130,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(87000,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(87870,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(88740,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in te</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(89610,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in tex</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(90480,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(91350,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(92220,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(93090,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">ec</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(93960,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">ech</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(94830,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(95700,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(96570,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(97440,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;H</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(98310,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;He</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(99180,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hel</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(100050,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hell</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(100920,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(101790,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(102660,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello w</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(103530,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello wo</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(104400,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello wor</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(105270,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello worl</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(106140,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(107010,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(107880,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(108750,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; |</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(109620,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(110490,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(111360,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(112230,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(113100,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(113970,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(114840,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(115710,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(116580,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(117450,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(118320,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(119190,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(120060,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(120930,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer lla</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(121800,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(122670,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(123540,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(124410,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(125280,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(126150,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 en</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(127020,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 en</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(127890,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 enc</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(128760,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 enco</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(129630,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encod</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(130500,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(131370,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(132240,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode -</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(133110,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(133980,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --o</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(134850,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --ou</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(135720,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --out</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(136590,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --outp</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(137460,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --outpu</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(138330,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(139200,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(140070,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output j</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(140940,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output js</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(141810,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output jso</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(142680,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(143550,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(144420,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json |</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(145290,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(146160,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | j</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(147030,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(147900,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(148770,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(149640,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq le</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(150510,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq len</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(151380,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq leng</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(152250,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq lengt</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(153120,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(153990,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0"></tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(154860,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(155730,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ci" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(156600,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(157470,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">#</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(158340,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(159210,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(160080,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># R</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(160950,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Ra</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(161820,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Ra</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(162690,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(163560,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(164430,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(165300,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(166170,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(167040,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(167910,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(168780,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(169650,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(170520,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(171390,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token c</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(172260,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token co</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(173130,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token cou</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(174000,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token coun</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(174870,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(175740,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(176610,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(177480,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (n</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(178350,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(179220,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(180090,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no s</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(180960,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no sp</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(181830,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no spe</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(182700,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no spec</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(183570,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no speci</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(184440,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no specia</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(185310,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(186180,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(187050,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(187920,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(188790,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(189660,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(190530,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(191400,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(192270,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(193140,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text><text y="132.6" xml:space="preserve"><tspan x="0" class="t">&gt;</tspan></text></g><g transform="translate(194010,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(194880,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(195750,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(196620,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(197490,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(198360,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(199230,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(200100,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(200970,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(201840,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(202710,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(203580,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(204450,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(205320,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer lla</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(206190,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(207060,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(207930,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(208800,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(209670,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(210540,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 en</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(211410,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 enc</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(212280,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 enco</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(213150,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encod</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(214020,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(214890,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(215760,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode -</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(216630,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(217500,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --b</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(218370,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bo</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(219240,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(220110,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(220980,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=f</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(221850,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=fa</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(222720,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=fa</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(223590,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=fal</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(224460,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=fals</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(225330,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(226200,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(227070,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false -</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(227940,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(228810,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(229680,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eo</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(230550,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(231420,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(232290,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=f</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(233160,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=fa</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(234030,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=fal</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(234900,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=fals</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(235770,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(236640,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(237510,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(238380,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;H</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(239250,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;He</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(240120,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hel</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(240990,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hell</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(241860,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(242730,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(243600,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello w</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(244470,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello wo</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(245340,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello wor</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(246210,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello worl</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(247080,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello world</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(247950,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello world&#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(248820,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello world&#39;</tspan></text><text y="149.6" xml:space="preserve"><tspan x="0"></tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(249690,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello world&#39;</tspan></text><text y="149.6" xml:space="preserve"><tspan x="0"></tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan><tspan class="t">906 1917</tspan></text><text y="166.6" xml:space="preserve"><tspan x="0" class="t">&gt;</tspan></text></g><g transform="translate(250560,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello world&#39;</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">9906 1917</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(251430,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize Python code</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;def add(a, b): return a + b&#39; | tokenizer llama3</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">128000 755 923 2948 11 293 1680 471 264 489 293 198 128001</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Count tokens in text</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">echo &#39;Hello world&#39; | tokenizer llama3 encode --output json | jq length</tspan></text><text y="98.6" xml:space="preserve"><tspan x="0" class="t">5</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Raw token count (no special tokens)</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 encode --bos=false --eos=false &#39;Hello world&#39;</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">9906 1917</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ci" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(252300,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(253170,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(254040,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Te</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(254910,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Te</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(255780,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Tes</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(256650,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(257520,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(258390,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(259260,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test e</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(260130,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test en</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(261000,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test enc</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(261870,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test enc</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(262740,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test enco</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(263610,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encod</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(264480,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encodi</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(265350,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encodin</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(266220,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encodin</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(267090,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(267960,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(268830,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(269700,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/d</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(270570,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/de</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(271440,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/dec</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(272310,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/deco</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(273180,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decod</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(274050,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decodi</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(274920,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decodin</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(275790,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(276660,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(277530,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(278400,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(279270,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(280140,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(281010,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(281880,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(282750,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(283620,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(284490,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(285360,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(286230,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(287100,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(287970,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer lla</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(288840,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(289710,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(290580,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(291450,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(292320,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(293190,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;T</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(294060,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Te</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(294930,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Tes</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(295800,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(296670,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(297540,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test d</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(298410,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test da</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(299280,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test dat</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(300150,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(301020,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39;</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(301890,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(302760,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(303630,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; |</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(304500,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(305370,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(306240,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(307110,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(307980,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(308850,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(309720,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(310590,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(311460,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(312330,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(313200,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(314070,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(314940,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(315810,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(316680,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(317550,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer lla</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(318420,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(319290,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(320160,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(321030,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(321900,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(322770,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 d</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(323640,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 d</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(324510,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 de</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(325380,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 dec</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(326250,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 dec</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(327120,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 deco</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(327990,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decod</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(328860,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(329730,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan x="0"></tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(330600,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(331470,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t ci" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(332340,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t">#</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(333210,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(334080,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># T</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(334950,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># T</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(335820,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># To</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(336690,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(337560,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(338430,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(339300,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(340170,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(341040,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(341910,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(342780,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(343650,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(344520,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(345390,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(346260,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer i</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(347130,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer i</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(348000,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer in</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(348870,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer inf</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(349740,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer inf</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(350610,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(351480,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text><text y="64.6" xml:space="preserve"><tspan x="0" class="t">&gt;</tspan></text></g><g transform="translate(352350,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(353220,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(354090,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(354960,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(355830,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(356700,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(357570,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(358440,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(359310,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(360180,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(361050,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(361920,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(362790,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer l</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(363660,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer ll</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(364530,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer lla</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(365400,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llam</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(366270,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(367140,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(368010,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(368880,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 i</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(369750,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 in</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(370620,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 inf</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(371490,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(372360,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(373230,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info |</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(374100,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(374970,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | h</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(375840,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | he</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(376710,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | hea</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(377580,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | hea</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(378450,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(379320,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(380190,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(381060,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(381930,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan x="0"></tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(382800,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(383670,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ci" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(384540,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">#</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(385410,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(386280,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># I</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(387150,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># In</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(388020,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Ins</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(388890,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Inst</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(389760,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Insta</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(390630,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Instal</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(391500,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(392370,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install:</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(393240,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(394110,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: b</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(394980,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: br</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(395850,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: bre</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(396720,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(397590,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(398460,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew i</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(399330,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew in</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(400200,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew ins</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(401070,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew inst</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(401940,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew insta</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(402810,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew instal</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(403680,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(404550,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(405420,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install a</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(406290,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install ag</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(407160,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install age</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(408030,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agen</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(408900,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agent</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(409770,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agents</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(410640,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentst</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(411510,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentsta</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(412380,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstat</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(413250,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstati</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(414120,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstatio</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(414990,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(415860,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(416730,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(417600,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/ta</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(418470,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(419340,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(420210,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/t</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(421080,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/to</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(421950,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tok</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(422820,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/toke</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(423690,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/token</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(424560,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tokeni</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(425430,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tokeniz</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(426300,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tokenize</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(427170,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Test encoding/decoding</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="98.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tokenizer</tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(428040,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tokenizer</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ca" style="fill:#8b949e;">█</tspan></text></g><g transform="translate(428910,0)"><text y="13.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 &#39;Test data&#39; | tokenizer llama3 decode</tspan></text><text y="30.6" xml:space="preserve"><tspan class="t">&lt;|begin_of_text|&gt;Test data&lt;|end_of_text|&gt;</tspan><tspan class="t p">&gt; </tspan><tspan class="t"># Tokenizer info</tspan></text><text y="47.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t">tokenizer llama3 info | head -8</tspan></text><text y="64.6" xml:space="preserve"><tspan class="t">Llama 3 Tokenizer Information</tspan></text><text y="81.6" xml:space="preserve"><tspan class="t">=============================</tspan></text><text y="115.6" xml:space="preserve"><tspan class="t">Model Details:</tspan></text><text y="132.6" xml:space="preserve"><tspan class="t">  Model Type:        Llama 3 (Meta)</tspan></text><text y="149.6" xml:space="preserve"><tspan class="t">  Tokenizer Type:    Byte-level BPE</tspan></text><text y="166.6" xml:space="preserve"><tspan class="t">  Vocabulary Size:   128256 tokens</tspan></text><text y="183.6" xml:space="preserve"><tspan class="t">  Regular Tokens:    128000</tspan></text><text y="200.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t"># Install: brew install agentstation/tap/tokenizer</tspan></text><text y="217.6" xml:space="preserve"><tspan class="t p">&gt; </tspan><tspan class="t ci" style="fill:#8b949e;">█</tspan></text></g></g></svg></svg>
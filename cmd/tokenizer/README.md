# Tokenizer CLI

A command-line interface for tokenizing text using various language model tokenizers.

## Installation

```bash
go install github.com/agentstation/tokenizer/cmd/tokenizer@latest
```

Or build from source:

```bash
go build -o tokenizer ./cmd/tokenizer
```

## Usage

The tokenizer CLI uses a subcommand structure where each tokenizer implementation is a subcommand.

### Basic Commands

```bash
# Encode text to token IDs
tokenizer llama3 encode "Hello, world!"
# Output: 128000 9906 11 1917 0 128001

# Decode token IDs back to text
tokenizer llama3 decode 128000 9906 11 1917 0 128001
# Output: <|begin_of_text|>Hello, world!<|end_of_text|>

# Get tokenizer information
tokenizer llama3 info
```

### Encoding Options

```bash
# Encode without special tokens
tokenizer llama3 encode --bos=false --eos=false "Hello, world!"
# Output: 9906 11 1917 0

# Different output formats
tokenizer llama3 encode -o json "Hello, world!"
# Output: [128000,9906,11,1917,0,128001]

tokenizer llama3 encode -o newline "Hello, world!"
# Output: (one token per line)
# 128000
# 9906
# 11
# 1917
# 0
# 128001
```

### Piping and Streaming

```bash
# Pipe text to encode
echo "Hello, world!" | tokenizer llama3 encode

# Pipe tokens to decode
echo "128000 9906 11 1917 0 128001" | tokenizer llama3 decode

# Round-trip encoding and decoding
tokenizer llama3 encode "test" | tokenizer llama3 decode

# Stream large files
cat large_file.txt | tokenizer llama3 stream
```

### Streaming Mode

For processing large files or real-time input:

```bash
# Basic streaming
tokenizer llama3 stream < input.txt

# Custom buffer settings
tokenizer llama3 stream --buffer-size=8192 --max-buffer=2097152 < large_file.txt

# Stream without special tokens
tokenizer llama3 stream --bos=false --eos=false < input.txt
```

## Available Tokenizers

### llama3

Meta's Llama 3 tokenizer with 128,256 tokens (128,000 regular + 256 special tokens).

**Commands:**
- `encode` - Convert text to token IDs
- `decode` - Convert token IDs to text  
- `stream` - Process text in streaming mode
- `info` - Display tokenizer information

## Examples

### Tokenize a file

```bash
# Tokenize entire file
tokenizer llama3 encode < document.txt > tokens.txt

# Count tokens in a file
tokenizer llama3 encode < document.txt | wc -w
```

### Batch processing

```bash
# Process multiple files
for file in *.txt; do
    echo "Tokenizing $file..."
    tokenizer llama3 encode < "$file" > "${file%.txt}.tokens"
done
```

### Integration with other tools

```bash
# Use with jq for JSON processing
tokenizer llama3 encode -o json "Hello" | jq length

# Extract specific tokens
tokenizer llama3 encode "Hello, world!" | awk '{print $2}'
```

## Future Tokenizers

The CLI is designed to support multiple tokenizers. Future additions may include:
- GPT-2/GPT-3 tokenizers
- BERT tokenizer
- SentencePiece tokenizers
- Custom tokenizers

Each tokenizer will follow the same subcommand pattern:
```bash
tokenizer [tokenizer-name] [command] [options]
```

<!-- gomarkdoc:embed:start -->

<!-- Code generated by gomarkdoc. DO NOT EDIT -->

# tokenizer

```go
import "github.com/agentstation/tokenizer/cmd/tokenizer"
```

## Index



Generated by [gomarkdoc](<https://github.com/princjef/gomarkdoc>)


<!-- gomarkdoc:embed:end -->